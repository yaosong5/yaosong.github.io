<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="纵浪大化中，不喜亦不悲"><title>Spark-on-Yarn源码解析(四)Spark业务代码的执行及其任务分配调度stage划分 | 钢铁锅</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Spark-on-Yarn源码解析(四)Spark业务代码的执行及其任务分配调度stage划分</h1><a id="logo" href="/.">钢铁锅</a><p class="description">应尽便须尽，无复独多虑</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Spark-on-Yarn源码解析(四)Spark业务代码的执行及其任务分配调度stage划分</h1><div class="post-meta">Sep 4, 2018<span> | </span><span class="category"><a href="/categories/Spark-On-Yarn/">Spark-On-Yarn</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#看看自定义的类"><span class="toc-number">1.</span> <span class="toc-text">看看自定义的类</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#sparkContext的初始化"><span class="toc-number">2.</span> <span class="toc-text">sparkContext的初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#SparkEnv"><span class="toc-number">2.1.</span> <span class="toc-text">SparkEnv</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TaskScheduler"><span class="toc-number">2.2.</span> <span class="toc-text">TaskScheduler</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#启动TaskScheduler"><span class="toc-number">2.2.1.</span> <span class="toc-text">启动TaskScheduler</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DAGScheduler"><span class="toc-number">2.3.</span> <span class="toc-text">DAGScheduler</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#RDD的构建过程"><span class="toc-number">3.</span> <span class="toc-text">RDD的构建过程</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#作业提交"><span class="toc-number">4.</span> <span class="toc-text">作业提交</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#任务流转"><span class="toc-number">5.</span> <span class="toc-text">任务流转</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#stage-的划分"><span class="toc-number">5.1.</span> <span class="toc-text">stage 的划分</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#任务的生成"><span class="toc-number">5.2.</span> <span class="toc-text">任务的生成</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TaskScheduler-amp-amp-TaskSchedulerBackend"><span class="toc-number">5.3.</span> <span class="toc-text">TaskScheduler &amp;&amp; TaskSchedulerBackend</span></a></li></ol></li></ol></div></div><div class="post-content"><p>spark-on-yarn系列<br><a href="&quot;http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(一">Spark-on-Yarn 源码解析 (一)Yarn 任务解析</a>Yarn任务解析/“)<br><a href="&quot;http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(二">Spark-on-Yarn 源码解析 (二)Spark-Submit 解析</a>Spark-Submit解析/“)<br><a href="&quot;http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(三">Spark-on-Yarn 源码解析 (三)client 做的事情</a>client做的事情/“)<br><a href="&quot;http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(四">Spark-on-Yarn 源码解析 (四)Spark 业务代码的执行及其任务分配调度 stage 划分</a>Spark业务代码的执行及其任务分配调度stage划分/“)</p>
<h1 id="看看自定义的类"><a href="#看看自定义的类" class="headerlink" title="看看自定义的类"></a>看看自定义的类</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"yaoWordCount"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">var</span> hadoopRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(args(<span class="number">0</span>))</span><br><span class="line">    <span class="keyword">var</span> hdfsRDD: <span class="type">RDD</span>[<span class="type">String</span>] = hadoopRDD.flatMap(_.split(<span class="string">""</span>))</span><br><span class="line">    <span class="comment">//单词和出现的次数，构建RDD并且调用了他的Transformation</span></span><br><span class="line">    <span class="comment">//返回的是一个hadoopRDD</span></span><br><span class="line">    <span class="comment">//transFormation都是返回的RDD</span></span><br><span class="line">    <span class="keyword">var</span> wordAndCount: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = hdfsRDD.map((_, <span class="number">1</span>))</span><br><span class="line">    <span class="comment">//创建RDD 这里面有两个RDD,一个是hadoopRDD，然后会生成一个paritionRDD</span></span><br><span class="line">    <span class="comment">//savaasTextfile还会产生一个RDD,因为会调用mapPartitons</span></span><br><span class="line">    <span class="comment">//调用RDD的action 开始真正提交任务</span></span><br><span class="line">    <span class="keyword">var</span> reducedRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordAndCount.reduceByKey(_ + _)</span><br><span class="line">    reducedRDD.saveAsTextFile(args(<span class="number">1</span>))</span><br><span class="line">    <span class="comment">//关闭saprkContext资源</span></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h1 id="sparkContext的初始化"><a href="#sparkContext的初始化" class="headerlink" title="sparkContext的初始化"></a>sparkContext的初始化</h1><p>对于Spark程序入口为SparkContext,当我们使用spark-submit/spark-shell等命令来启动一个客户端,客户端与集群需要建立链接，建立的这个链接对象就叫做sparkContext，只有这个对象创建成功才标志这这个客户端与spark集群链接成功。现就将从SparkContext展开来描述一下Spark的任务启动和执行流程。<br>SparkContext 完成了以下几个主要的功能：<br>（1）创建 RDD，通过类似 textFile 等的方法。<br>（2）与资源管理器交互，通过 runJob 等方法启动应用。<br>（3）创建 DAGScheduler、TaskScheduler 等。 </p>
<p>在SparkContext类中，SparkContext主构造器主要做</p>
<p>我们看一下SparkContext的主构造器</p>
<ul>
<li>调用CreateSparkEnv方法创建SparkEnv(将driver的信息，url，ip等都封装)，SparkEnv中有一个对象ActorSystem</li>
<li>创建TaskScheduler ，根据提交任务的URL（如：spark://(.*)”，local[1]等，去创建TaskSchedulerImpl ，然后再创建SparkDeploySchedulerBackend(先后创建driverActor和clientActor)</li>
<li>创建DAGScheduler</li>
<li>TaskScheduler启动，TaskScheduler.start()</li>
</ul>
<h2 id="SparkEnv"><a href="#SparkEnv" class="headerlink" title="SparkEnv"></a>SparkEnv</h2><p>最终将driver的host,port端口等各种信息都封装到里面</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">new</span> <span class="type">SparkEnv</span>(</span><br><span class="line">  executorId,</span><br><span class="line">  actorSystem,</span><br><span class="line">  serializer,</span><br><span class="line">  closureSerializer,</span><br><span class="line">  cacheManager,</span><br><span class="line">  mapOutputTracker,</span><br><span class="line">  shuffleManager,</span><br><span class="line">  broadcastManager,</span><br><span class="line">  blockTransferService,</span><br><span class="line">  blockManager,</span><br><span class="line">  securityManager,</span><br><span class="line">  httpFileServer,</span><br><span class="line">  sparkFilesDir,</span><br><span class="line">  metricsSystem,</span><br><span class="line">  shuffleMemoryManager,</span><br><span class="line">  outputCommitCoordinator,</span><br><span class="line">  conf)</span><br></pre></td></tr></table></figure>
<h2 id="TaskScheduler"><a href="#TaskScheduler" class="headerlink" title="TaskScheduler"></a>TaskScheduler</h2><p>在SparkContext类中可以看到，TaskScheduler根据url类型匹配创建TaskSchedulerImpl</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">//TODO 根据提交任务时指定的URL创建相应的TaskScheduler</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createTaskScheduler</span></span>(</span><br><span class="line">      sc: <span class="type">SparkContext</span>,</span><br><span class="line">      master: <span class="type">String</span>): (<span class="type">SchedulerBackend</span>, <span class="type">TaskScheduler</span>) = &#123;</span><br><span class="line">      ...</span><br><span class="line">      <span class="keyword">case</span> <span class="string">"yarn-standalone"</span> | <span class="string">"yarn-cluster"</span> =&gt;</span><br><span class="line">...</span><br><span class="line">        <span class="keyword">val</span> scheduler = <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="keyword">val</span> clazz = <span class="type">Class</span>.forName(<span class="string">"org.apache.spark.scheduler.cluster.YarnClusterScheduler"</span>)</span><br><span class="line">          <span class="keyword">val</span> cons = clazz.getConstructor(classOf[<span class="type">SparkContext</span>])</span><br><span class="line">          cons.newInstance(sc).asInstanceOf[<span class="type">TaskSchedulerImpl</span>]</span><br><span class="line">          &#125;</span><br><span class="line">      ...</span><br><span class="line">        <span class="keyword">val</span> backend = <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="keyword">val</span> clazz =</span><br><span class="line">            <span class="type">Class</span>.forName(<span class="string">"org.apache.spark.scheduler.cluster.YarnClusterSchedulerBackend"</span>)</span><br><span class="line">          <span class="keyword">val</span> cons = clazz.getConstructor(classOf[<span class="type">TaskSchedulerImpl</span>], classOf[<span class="type">SparkContext</span>])</span><br><span class="line">          cons.newInstance(scheduler, sc).asInstanceOf[<span class="type">CoarseGrainedSchedulerBackend</span>]</span><br><span class="line">        &#125; </span><br><span class="line">        scheduler.initialize(backend)</span><br><span class="line">        (backend, scheduler)</span><br><span class="line">        ....</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<!--more-->
<p>可知<br>TaskScheduler 的实现类<code>org.apache.spark.scheduler.cluster.YarnScheduler</code><br>TaskSchedulerBacked 的实现类为<code>org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend</code><br>且TaskScheduler对TaskSchedulerBacked保持了引用<br>scheduler.initialize(backend)</p>
<h3 id="启动TaskScheduler"><a href="#启动TaskScheduler" class="headerlink" title="启动TaskScheduler"></a>启动TaskScheduler</h3><p>在Spark的构造函数中,会启动TaskScheduler</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">taskScheduler.start()</span><br></pre></td></tr></table></figure>
<p>可以看到继承关系</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">YarnClusterScheduler</span>(<span class="params">sc: <span class="type">SparkContext</span></span>) <span class="keyword">extends</span> <span class="title">YarnScheduler</span>(<span class="params">sc</span>) </span></span><br><span class="line"><span class="class"><span class="title">private</span>[spark] <span class="title">class</span> <span class="title">YarnScheduler</span>(<span class="params">sc: <span class="type">SparkContext</span></span>) <span class="keyword">extends</span> <span class="title">TaskSchedulerImpl</span>(<span class="params">sc</span>)</span></span><br></pre></td></tr></table></figure>
<p>可以跟踪到，start方法最终调用的是TaskSchedulerImpl里面start方法，在start方法里面</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">start</span></span>() &#123;</span><br><span class="line">    <span class="comment">//TODO 首先调用SparkDeploySchedulerBackend的start方法</span></span><br><span class="line">    backend.start()</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>,这里的backend就是YarnClusterSchedulerBackend，而这个最终继承的是CoarseGrainedSchedulerBackend中start方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">start</span></span>() &#123;</span><br><span class="line">...</span><br><span class="line">  driverActor = actorSystem.actorOf(</span><br><span class="line">    <span class="type">Props</span>(<span class="keyword">new</span> <span class="type">DriverActor</span>(properties)), name = <span class="type">CoarseGrainedSchedulerBackend</span>.<span class="type">ACTOR_NAME</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>获取到spark的配置信息后，会创建driverActor</p>
<h2 id="DAGScheduler"><a href="#DAGScheduler" class="headerlink" title="DAGScheduler"></a>DAGScheduler</h2><p>在SparkContext的构造函数中，会创建DAGScheduler</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dagScheduler= <span class="keyword">new</span> <span class="type">DAGScheduler</span>(<span class="keyword">this</span>)</span><br></pre></td></tr></table></figure>
<p>在DAGScheduler构造函数中</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(sc: <span class="type">SparkContext</span>) = <span class="keyword">this</span>(sc, sc.taskScheduler)</span><br></pre></td></tr></table></figure>
<p>可以看到DAGScheduler对TaskScheduler保持了引用</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DAGScheduler</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    private[scheduler] val sc: <span class="type">SparkContext</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private[scheduler] val taskScheduler: <span class="type">TaskScheduler</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    listenerBus: <span class="type">LiveListenerBus</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    mapOutputTracker: <span class="type">MapOutputTrackerMaster</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    blockManagerMaster: <span class="type">BlockManagerMaster</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    env: <span class="type">SparkEnv</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    clock: <span class="type">Clock</span> = new <span class="type">SystemClock</span>(</span>))</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">  ......</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>mapOutputTracker：是运行在 Driver 端管理 shuffle 的中间输出位置信息的。 </li>
<li>blockManagerMaster：也是运行在 Driver 端的，它是管理整个 Job 的 Bolck 信息。</li>
</ul>
<h1 id="RDD的构建过程"><a href="#RDD的构建过程" class="headerlink" title="RDD的构建过程"></a>RDD的构建过程</h1><p>其中hadoopRDD，hdfsRDD，wordRDD，reduceRDD是经过一系列transformation装换rdd，只有等到action时，才会触发数据的流转</p>
<p>该例的action为saveAsTextFile调用链为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">saveAsTextFile()</span><br><span class="line">    saveAsHadoopFile()</span><br><span class="line">         saveAsHadoopFile（重载函数）</span><br><span class="line">                 saveAsHadoopDataset()</span><br><span class="line">                     runJob()之间会调用几个重载函数</span><br><span class="line">                     dagScheduler.runJob()最终调用</span><br></pre></td></tr></table></figure>
<h1 id="作业提交"><a href="#作业提交" class="headerlink" title="作业提交"></a>作业提交</h1><h1 id="任务流转"><a href="#任务流转" class="headerlink" title="任务流转"></a>任务流转</h1><p>首先注意区分 2 个概述：<br>job: 每个 action 都是执行 runJob 方法，可以将之视为一个 job。<br>stage：在这个 job 内部，会根据宽依赖，划分成多个 stage。</p>
<p>在action触发后，最最终调用的是DAGScheduler.runJob()</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span><br></pre></td></tr></table></figure>
<p>而runJob() 的核心代码为：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)</span><br></pre></td></tr></table></figure>
<p>即调用 submitJob 方法，我们进一步看看 submitJob()</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">submitJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](</span><br><span class="line">      rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span><br><span class="line">      partitions: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">      callSite: <span class="type">CallSite</span>,</span><br><span class="line">      resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>,</span><br><span class="line">      properties: <span class="type">Properties</span>): <span class="type">JobWaiter</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">....    </span><br><span class="line">    <span class="keyword">val</span> jobId = nextJobId.getAndIncrement()</span><br><span class="line">.....</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> waiter = <span class="keyword">new</span> <span class="type">JobWaiter</span>(<span class="keyword">this</span>, jobId, partitions.size, resultHandler)</span><br><span class="line">    eventProcessLoop.post(<span class="type">JobSubmitted</span>(</span><br><span class="line">      jobId, rdd, func2, partitions.toArray, callSite, waiter,</span><br><span class="line">      <span class="type">SerializationUtils</span>.clone(properties)))</span><br><span class="line">    waiter</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>submitJob() 方法主要完成了以下 3 个工作： </p>
<ul>
<li>获取一个新的 jobId </li>
<li>生成一个 JobWaiter，它会监听 Job 的执行状态，而 Job 是由多个 Task 组成的，因此只有当 Job 的所有 Task 均已完成，Job 才会标记成功 </li>
<li>最后调用 eventProcessLoop.post() 将 Job 提交到一个队列中，等待处理。这是一个典型的生产者消费者模式。这些消息都是通过 handleJobSubmitted 来处理。</li>
</ul>
<p>简单看一下 handleJobSubmitted 是如何被调用的。<br>首先是 DAGSchedulerEventProcessLoop#onReceive 调用 </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//TODO 通过模式匹配判断事件的类型 比如任务提交，作业取消 ...</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = event <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="comment">//TODO 提交计算任务</span></span><br><span class="line">  <span class="keyword">case</span> <span class="type">JobSubmitted</span>(jobId, rdd, func, partitions, allowLocal, callSite, listener, properties) =&gt;</span><br><span class="line">    <span class="comment">//todo 调用dagScheduler的handlerJobSubmitted方法处理</span></span><br><span class="line">    dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, allowLocal, callSite,  listener, properties)</span><br><span class="line">  ... ...</span><br></pre></td></tr></table></figure>
<p>DAGSchedulerEventProcessLoop 是 EventLoop 的子类，它重写了 EventLoop 的 onReceive 方法。以后再分析这个 EventLoop。<br>onReceive 会调用 handleJobSubmitted。</p>
<h2 id="stage-的划分"><a href="#stage-的划分" class="headerlink" title="stage 的划分"></a>stage 的划分</h2><p>刚才说到 handleJobSubmitted 会从 eventProcessLoop 中取出 Job 来进行处理，处理的第一步就是将 Job 划分成不同的 stage。handleJobSubmitted 主要 2 个工作，一是进行 stage 的划分，这是这部分要介绍的内容；二是创建一个 activeJob，并生成一个任务，这在下一小节介绍。</p>
<p>还是先看看调用链</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">handleJobSubmitted</span><br><span class="line">    -&gt;newStage()</span><br><span class="line">      -&gt;getParentStages()//此处会遍历RDD所有依赖</span><br><span class="line">         -&gt;getShuffleMapStage()//如果是ShuffleDependency（宽依赖，获取到一个Map）</span><br><span class="line">             -&gt;newOrUsedStage()//这就可以解释我们常说的遇到宽依赖就会划分stage，并且返回stage</span><br></pre></td></tr></table></figure>
<p>所以最终返回的是一个拥有款依赖的           </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">handleJobSubmitted</span></span>(jobId: <span class="type">Int</span>,</span><br><span class="line">    finalRDD: <span class="type">RDD</span>[_],</span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,</span><br><span class="line">    partitions: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">    callSite: <span class="type">CallSite</span>,</span><br><span class="line">    listener: <span class="type">JobListener</span>,</span><br><span class="line">    properties: <span class="type">Properties</span>) &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">//todo 重要：该方法用于划分stage，主要依赖的是finalStage</span></span><br><span class="line">     finalStage = newStage(finalRDD, partitions.size, <span class="type">None</span>, jobId, callSite)</span><br><span class="line">    .....</span><br><span class="line">  <span class="comment">//TODO 集群模式</span></span><br><span class="line">    activeJobs += job</span><br><span class="line">    ......</span><br><span class="line">  <span class="comment">//todo 提交stage</span></span><br><span class="line">    submitStage(finalStage)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//TODO  开始向集群提交还在等待的stage</span></span><br><span class="line">  submitWaitingStages()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>getParentStages()。<br>因为是从最终的 stage 往回推算的，这需要计算最终 stage 所依赖的各个 stage。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//TODO 用于获取父stage</span></span><br><span class="line"> <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getParentStages</span></span>(rdd: <span class="type">RDD</span>[_], jobId: <span class="type">Int</span>): <span class="type">List</span>[<span class="type">Stage</span>] = &#123;</span><br><span class="line">   <span class="keyword">val</span> parents = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">Stage</span>]</span><br><span class="line">   <span class="keyword">val</span> waitingForVisit = <span class="keyword">new</span> <span class="type">Stack</span>[<span class="type">RDD</span>[_]]</span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">visit</span></span>(r: <span class="type">RDD</span>[_]) &#123;</span><br><span class="line">     <span class="keyword">if</span> (!visited(r)) &#123;</span><br><span class="line">       visited + r</span><br><span class="line">       <span class="keyword">for</span> (dep &lt;- r.dependencies) &#123;</span><br><span class="line">         dep <span class="keyword">match</span> &#123;</span><br><span class="line">           <span class="keyword">case</span> shufDep: <span class="type">ShuffleDependency</span>[_, _, _] =&gt;</span><br><span class="line">             <span class="comment">//TODO 把宽依赖传进去，获得父stage</span></span><br><span class="line">             parents += getShuffleMapStage(shufDep, jobId)</span><br><span class="line">           <span class="keyword">case</span> _ =&gt;</span><br><span class="line">             waitingForVisit.push(dep.rdd)</span><br><span class="line">         &#125;</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   waitingForVisit.push(rdd)</span><br><span class="line">   <span class="keyword">while</span> (!waitingForVisit.isEmpty) &#123;</span><br><span class="line">     visit(waitingForVisit.pop())</span><br><span class="line">   &#125;</span><br><span class="line">   parents.toList</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<h2 id="任务的生成"><a href="#任务的生成" class="headerlink" title="任务的生成"></a>任务的生成</h2><p>回到 handleJobSubmitted 中的代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">submitStage(finalStage)</span><br></pre></td></tr></table></figure>
<p>submitStage 会提交 finalStage，如果这个 stage 的某些 parentStage 未提交，则递归调用 submitStage()，直至所有的 stage 均已计算完成。</p>
<p>submitStage() 会调用 submitMissingTasks():</p>
<p>submitMissingTasks(stage, jobId.get)</p>
<p>而 submitMissingTasks() 会完成 DAGScheduler 最后的工作：它判断出哪些 Partition 需要计算，为每个 Partition 生成 Task，然后这些 Task 就会封闭到 TaskSet</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//TODO  DAG提交stage  根据最后一个stage  开始找到第一个stage递归提交stage</span></span><br><span class="line"> <span class="comment">/** Submits stage, but first recursively submits any missing parents. */</span></span><br><span class="line"> <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitStage</span></span>(stage: <span class="type">Stage</span>) &#123;</span><br><span class="line">   <span class="keyword">val</span> jobId = activeJobForStage(stage)</span><br><span class="line">   <span class="keyword">if</span> (jobId.isDefined) &#123;</span><br><span class="line">     </span><br><span class="line">     <span class="keyword">if</span> (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123;</span><br><span class="line">       <span class="comment">//TODO 获取他的父stage 没有提交的stage</span></span><br><span class="line">       <span class="keyword">val</span> missing = getMissingParentStages(stage).sortBy(_.id)</span><br><span class="line">       <span class="comment">//todo 判断父stage是否为空，为空就以为着他是第一stage</span></span><br><span class="line">       <span class="keyword">if</span> (missing == <span class="type">Nil</span>) &#123;</span><br><span class="line">      <span class="comment">//TODO 开始提交最前面的stage, DAG提交stage给TaskScheduler 会将stage转换成taskSet</span></span><br><span class="line">         submitMissingTasks(stage, jobId.get)</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">         <span class="comment">//TODO 有父stage  就递归提交</span></span><br><span class="line">         <span class="keyword">for</span> (parent &lt;- missing) &#123;</span><br><span class="line">           submitStage(parent)</span><br><span class="line">         &#125;</span><br><span class="line">         waitingStages += stage</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br><span class="line">   &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     abortStage(stage, <span class="string">"No active job for stage "</span> + stage.id)</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>submitMissingTasks在最后提交给 TaskScheduler 进行处理</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">//TODO  DAG提交stage给TaskScheduler 会将stage转换成taskSet</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitMissingTasks</span></span>(stage: <span class="type">Stage</span>, jobId: <span class="type">Int</span>) &#123;</span><br><span class="line">...</span><br><span class="line"><span class="comment">//TODO 创建多少个Task</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> tasks: <span class="type">Seq</span>[<span class="type">Task</span>[_]] = <span class="keyword">if</span> (stage.isShuffleMap) &#123;</span><br><span class="line">      partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">        <span class="comment">//TODO 数据存储的最佳位置   移动计算，而不是移动数据</span></span><br><span class="line">        <span class="keyword">val</span> locs = getPreferredLocs(stage.rdd, id)</span><br><span class="line">        <span class="keyword">val</span> part = stage.rdd.partitions(id)</span><br><span class="line">        <span class="comment">//TODO 从上游拉取数据</span></span><br><span class="line">        <span class="keyword">new</span> <span class="type">ShuffleMapTask</span>(stage.id, taskBinary, part, locs)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> job = stage.resultOfJob.get</span><br><span class="line">      partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">        <span class="keyword">val</span> p: <span class="type">Int</span> = job.partitions(id)</span><br><span class="line">        <span class="keyword">val</span> part = stage.rdd.partitions(p)</span><br><span class="line">        <span class="keyword">val</span> locs = getPreferredLocs(stage.rdd, p)</span><br><span class="line">        <span class="comment">//TODO  将数据写入某个介质里面，nosql hdfs 等等</span></span><br><span class="line">        <span class="keyword">new</span> <span class="type">ResultTask</span>(stage.id, taskBinary, part, locs, id)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//TODO task的数量最好和分区数一样  如果分区数大于0</span></span><br><span class="line"> <span class="comment">//TODO task的数量最好和分区数一样  如果分区数大于0</span></span><br><span class="line">    <span class="keyword">if</span> (tasks.size &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      logInfo(<span class="string">"Submitting "</span> + tasks.size + <span class="string">" missing tasks from "</span> + stage + <span class="string">" ("</span> + stage.rdd + <span class="string">")"</span>)</span><br><span class="line">      stage.pendingTasks ++= tasks</span><br><span class="line"></span><br><span class="line">      <span class="comment">//TODO 调用taskScheduler的submitTasks提交taskSet 现在将task转换成一个array</span></span><br><span class="line">taskScheduler.submitTasks(<span class="keyword">new</span> <span class="type">TaskSet</span>(</span><br><span class="line">        tasks.toArray, stage.id, stage.latestInfo.attemptId, stage.firstJobId, properties))</span><br><span class="line">      stage.latestInfo.submissionTime = <span class="type">Some</span>(clock.getTimeMillis())</span><br><span class="line">      .....</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="TaskScheduler-amp-amp-TaskSchedulerBackend"><a href="#TaskScheduler-amp-amp-TaskSchedulerBackend" class="headerlink" title="TaskScheduler &amp;&amp; TaskSchedulerBackend"></a>TaskScheduler &amp;&amp; TaskSchedulerBackend</h2><p>上文分析到在 DAGScheduler 中最终会执行 taskScheduler.submitTasks() 方法，我们先简单看一下从这里开始往下的执行逻辑：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">①taskScheduler.submitTasks()</span><br><span class="line">    -&gt;②schedulableBuilder.addTaskSetManager() 调度模式，是先来先服务还是公平调度模式</span><br><span class="line">    -&gt;③CoarseGrainedSchedulerBackend.reviveOffers() 这个是向driverActor发送消息driverActor ! ReviveOffers</span><br><span class="line">        -&gt;④CoarseGrainedSchedulerBackend.receiveWithLogging 这是driverActor接收消息的部分</span><br><span class="line">            -&gt;⑤CoarseGrainedSchedulerBackend.makeOffers() //case ReviveOffers =&gt;makeOffers()</span><br><span class="line">        这个模式匹配会调用maksOffers方法</span><br><span class="line">                -&gt;⑥launchTasks()调用launchTask向Executor提交task</span><br><span class="line">                    -&gt;⑦ executorData.executorActor ! LaunchTask(new SerializableBuffer(serializedTask))向executor发送序列化好的task，发送一个Task</span><br></pre></td></tr></table></figure>
<p>步骤一、二中主要将这组<br>任务的 TaskSet 加入到一个 TaskSetManager 中。TaskSetManager 会根据数据就近原则为 task 分配计算资源，监控 task 的执行状态等，比如失败重试，推测执行等。<br>步骤三、四逻辑较为简单。<br>步骤五为每个 task 具体分配资源，它的输入是一个 Executor 的列表，输出是 TaskDescription 的二维数组。TaskDescription 包含了 TaskID, Executor ID 和 task 执行的依赖信息等。<br>步骤六、七就是将任务真正的发送到 executor 中执行了，并等待 executor 的状态返回。</p>
<p>​                 </p>
</div><div class="tags"><a href="/tags/原理/">原理</a><a href="/tags/Spark/">Spark</a></div><div class="post-nav"><a class="next" href="/2018/09/04/Spark-on-Yarn源码解析(三)client做的事情/">Spark-on-Yarn源码解析(三)client做的事情</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://gangtieguo.cn"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Spark-On-Yarn/">Spark-On-Yarn</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Spring/">Spring</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tool/">tool</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/博客/">博客</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/大数据/">大数据</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/安装部署/">安装部署</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/工程框架/">工程框架</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/快捷键/">快捷键</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/总结/">总结</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/框架/">框架</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/环境配置/">环境配置</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/碎片知识/">碎片知识</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/组件/">组件</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/语言/">语言</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/大数据/" style="font-size: 15px;">大数据</a> <a href="/tags/Docker/" style="font-size: 15px;">Docker</a> <a href="/tags/Jenkins/" style="font-size: 15px;">Jenkins</a> <a href="/tags/Docker-machine/" style="font-size: 15px;">Docker-machine</a> <a href="/tags/安装部署/" style="font-size: 15px;">安装部署</a> <a href="/tags/CDH/" style="font-size: 15px;">CDH</a> <a href="/tags/Hadoop/" style="font-size: 15px;">Hadoop</a> <a href="/tags/HBase/" style="font-size: 15px;">HBase</a> <a href="/tags/操作/" style="font-size: 15px;">操作</a> <a href="/tags/原理/" style="font-size: 15px;">原理</a> <a href="/tags/HDFS/" style="font-size: 15px;">HDFS</a> <a href="/tags/Hive/" style="font-size: 15px;">Hive</a> <a href="/tags/使用/" style="font-size: 15px;">使用</a> <a href="/tags/Shell/" style="font-size: 15px;">Shell</a> <a href="/tags/报表/" style="font-size: 15px;">报表</a> <a href="/tags/Json/" style="font-size: 15px;">Json</a> <a href="/tags/Scala/" style="font-size: 15px;">Scala</a> <a href="/tags/Kafka/" style="font-size: 15px;">Kafka</a> <a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/Mysql/" style="font-size: 15px;">Mysql</a> <a href="/tags/linux/" style="font-size: 15px;">linux</a> <a href="/tags/开发/" style="font-size: 15px;">开发</a> <a href="/tags/Ambari/" style="font-size: 15px;">Ambari</a> <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/SSH/" style="font-size: 15px;">SSH</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/Yarn/" style="font-size: 15px;">Yarn</a> <a href="/tags/SparkStreaming/" style="font-size: 15px;">SparkStreaming</a> <a href="/tags/SparkSQL/" style="font-size: 15px;">SparkSQL</a> <a href="/tags/RDD/" style="font-size: 15px;">RDD</a> <a href="/tags/ELK/" style="font-size: 15px;">ELK</a> <a href="/tags/es/" style="font-size: 15px;">es</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/FLINK/" style="font-size: 15px;">FLINK</a> <a href="/tags/命令/" style="font-size: 15px;">命令</a> <a href="/tags/Hue/" style="font-size: 15px;">Hue</a> <a href="/tags/技术/" style="font-size: 15px;">技术</a> <a href="/tags/zk/" style="font-size: 15px;">zk</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/Other/" style="font-size: 15px;">Other</a> <a href="/tags/快捷键/" style="font-size: 15px;">快捷键</a> <a href="/tags/Mac/" style="font-size: 15px;">Mac</a> <a href="/tags/Idea/" style="font-size: 15px;">Idea</a> <a href="/tags/Finder/" style="font-size: 15px;">Finder</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/09/04/Spark-on-Yarn源码解析(四)Spark业务代码的执行及其任务分配调度stage划分/">Spark-on-Yarn源码解析(四)Spark业务代码的执行及其任务分配调度stage划分</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/09/04/Spark-on-Yarn源码解析(三)client做的事情/">Spark-on-Yarn源码解析(三)client做的事情</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/09/04/Spark-on-Yarn源码解析(二)Spark-Submit解析/">Spark-on-Yarn源码解析(二)Spark-Submit解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/09/04/Spark-on-Yarn源码解析(一)Yarn任务解析/">Spark-on-Yarn源码解析(一)Yarn任务解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/20/MapReduce中Shuffle中的机制/">MapReduce中Shuffle中的机制</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/16/SparkSQL介绍/">SparkSQL介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/16/Spark-On-yarn/">Spark-On-Yarn模式</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/16/SparkStreaming介绍/">SparkStreaming介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/16/SparkRDD介绍/">SparkRDD介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/16/Hadoop零碎知识点/">Hadoop零碎知识点</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">钢铁锅.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>